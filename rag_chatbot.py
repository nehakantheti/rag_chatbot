# -*- coding: utf-8 -*-
"""RAG_Chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11s3idpXKpHBQofeRuFF2fJYZWmo6ABkT
"""

# !pip install langchain_google_genai
# !pip install docx2txt pypdf unstructured
# !pip install langchain_community
# !pip install --upgrade langchain-core
# !pip install langchain_chroma
# !pip install sentence_transformers

import langchain

print(langchain.__version__)

import google.generativeai as genai

import os
os.environ["GOOGLE_API_KEY"] = "GOOGLE_API_KEY"  # Replace with your actual Google API key
os.environ["OPENAI_API_KEY"] = "OPENAI_API_KEY"  # Replace with your actual OpenAI API key

os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = "LANGCHAIN_API_KEY"  # Replace with your actual LangChain API key
os.environ["LANGCHAIN_PROJECT"] = "default"

# from langchain_openai import ChatOpenAI
# from openai import RateLimitError
# import time
# import os

# # Ensure OpenAI API key is set
# # os.environ["OPENAI_API_KEY"] = "YOUR_OPENAI_API_KEY" # Make sure this is set correctly

# llm = ChatOpenAI(model='gpt-4o-mini')

# try:
#     llm_response = llm.invoke("Tell me a joke")
#     print(llm_response)
# except RateLimitError as e:
#     print(f"Rate limit exceeded: {e}")
#     print("Please check your OpenAI plan and billing details.")
#     # You could add a retry mechanism here with increasing delays
#     # For example: time.sleep(60) and then retry the invoke call.
# except Exception as e:
#     print(f"An unexpected error occurred: {e}")

genai.configure(api_key="GOOGLE_API_KEY")  # Replace with your actual Google API key

models = []

# List available models to see the correct names (optional, but good for debugging)
print("Available Gemini models supporting generateContent:")
for m in genai.list_models():
 if 'generateContent' in m.supported_generation_methods:
   print(m.name)
   models.append(m.name[7:])

print(models)

import pandas as pd
df = pd.DataFrame(models, columns=["Model Name"])
df.to_csv("models.csv", index=False)

# Use the correct model name for Gemini Pro, which is typically 'gemini-1.0-pro'
response=''
try:
    model = genai.GenerativeModel("gemini-2.0-flash") # Changed model name to gemini-1.0-pro
    response = model.generate_content("Tell me a joke")
    print(response.text)
except Exception as e:
    print(f"An error occurred with the Gemini model: {e}")

from langchain_core.output_parsers import StrOutputParser

output_parser = StrOutputParser()
output_parser.invoke(response.text)

"""# **Simple Chain**"""

from langchain_google_genai import ChatGoogleGenerativeAI

# Initialize the Gemini model as a LangChain Runnable
# Use the correct model name, e.g., "gemini-1.0-pro" or "gemma-7b-it"
# Ensure you have the GOOGLE_API_KEY environment variable set or pass api_key directly
google_llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", api_key="GOOGLE_API_KEY")  # Replace with your actual Google API key"

# Create the chain by piping the LangChain Google model with the output parser
# Each time this chain is called, all these processes get
chain = google_llm | output_parser

# Invoke the chain with a prompt
print(chain.invoke("Tell me a joke"))
# %%

"""# **Structured Output**"""

from typing import List
from pydantic import BaseModel, Field

class MobileReview(BaseModel):
  phone_model: str = Field(description="Name and model of the phone")
  rating: float = Field(description="Overall rating out of 5")
  pros: List[str] = Field(description="List of Positive aspects")
  cons: List[str] = Field(description="List of Negative aspects")
  summary: str = Field(description="Summary of the review")

google_llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", api_key="GOOGLE_API_KEY")  # Replace with your actual Google API key"

review_text = """
Just got my hands on the new Galaxy S21 and wow, this thing is slick! The screen is gorgeous, colors pop like crazy. Camera's insane too, ezpeecially at night - my Insta game's never been stronger.
Battery life's solid, lasts me all day, no problem.

Not gonna lie though, it's pretty pricey. And what's with ditching the charger? C'mon Samsung.
Also still getting used to the new button layout, keep hitting  Bixby by mistake

Overall, I'd say it's a solid 4 out of 5. Great phone, but a few annoying quirks keep it from being perfect. If you're due for an upgrade, definitely worth checking out!
"""

structured_llm = google_llm.with_structured_output(MobileReview)
output = structured_llm.invoke(review_text)
output

output.pros

"""# **Prompt Template in Langchain**"""

from langchain_core.prompts import ChatPromptTemplate
prompt = ChatPromptTemplate.from_template("Tell me a short hoke about {topic}")
prompt.invoke({"topic" : "programming"})

chain = prompt | google_llm | output_parser
chain.invoke({"topic":"programmer"})

# Putting everything together
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# Define prompt
prompt = ChatPromptTemplate.from_template("Tell me a short joke about {topic}")

# Initialise the LLM
llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", api_key="GOOGLE_API_KEY")  # Replace with your actual Google API key"

# Create output parser
output_parser = StrOutputParser()

# Compose chain
chain = prompt | llm | output_parser

# Use chain
result = chain.invoke({"topic":"programming"})
print(result)

"""# **LLM Messages**"""

from langchain_core.messages import HumanMessage, SystemMessage

#  the system message serves as a guide for the model on how to use the retrieved documents to answer a user's query.
system_message = SystemMessage(content="You are a helpful assistant that tells jokes")

human_message = HumanMessage(content='Tell me about programming')

google_llm.invoke([system_message, human_message])

# Putting system and human messages in to a template

template = ChatPromptTemplate([
    ("system", "You are a helpful assistant that tells jokes"),
    ("human", "Tell me about {topic}")
])

# Setting the prompt in the placeholder value
prompt_value = template.invoke({
    "topic":"programming"
})

# Displaying the prompt generated
prompt_value

# Invoking LLM with the predefined chain
google_llm.invoke(prompt_value)

from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_core.documents import Document

# Splitting text from sources into Docs

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 1000,
    chunk_overlap = 200,
    length_function = len,
    add_start_index = True
)

docs_loader = PyPDFLoader('/content/annualreport-2023.pdf')
docs = docs_loader.load()
print(docs)
splits = text_splitter.split_documents(docs)

print(f"Split the docs into {len(splits)} chunks.")

# Each chunk is also a document
splits[0]

splits[0].metadata

splits[0].page_content

# Loading Documents
def load_documents(folder_path: str) -> List[Document]:
  documents = []
  for filename in os.listdir(folder_path):
    if filename.endswith('.pdf'):
      file_path = os.path.join(folder_path, filename)
      loader = PyPDFLoader(file_path)
      documents.extend(loader.load())
  return documents

folder_path = "/content"
documents = load_documents(folder_path)

print(f"Loaded {len(documents)} documents from the folder.")
splits = text_splitter.split_documents(documents)
print(f"Split the docs into {len(splits)} chunks.")

# Embedding Documents
# Explicitly pass the API key to the embeddings constructor
embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", api_key="GOOGLE_API_KEY")  # Replace with your actual Google API key"
3
document_embeddings = embeddings.embed_documents([split.page_content for split in splits])

print(f"Created {len(document_embeddings)} embeddings from documents.")

document_embeddings[0]

from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings
embedding_function = SentenceTransformerEmbeddings(model_name='all-MiniLM-L6-v2')
document_embeddings = embedding_function.embed_documents([split.page_content for split in splits])

document_embeddings[0]

from langchain_chroma import Chroma

embedding_function = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key="GOOGLE_API_KEY")  # Replace with your actual Google API key"

collection_name = "my_collection"
vectorstore = Chroma.from_documents(collection_name=collection_name, documents=splits, embedding=embedding_function)
# Persist saves the database to the disk and data will be accessible even after the current program termination
# db.persist()

print("Vector store created and persisted to './chromadb'")

# Performing Similarity Search

query = "Tell about JPMC's Finance in brief."
search_results = vectorstore.similarity_search(query)
print(len(search_results))

print(f"Top 2 relevant chunks for the query : '{query}' are :")
for i, result in enumerate(search_results, 1):
  print(f"Result {i}:")
  print(f"Source: {result.metadata.get('source', 'Unknown')}")
  print(f"Content: {result.page_content}")
  print()

# Retriever object to get all the relevant docs from vector database
retriever = vectorstore.as_retriever(search_kwargs={"k":2})
retriever.invoke("Tell about JPMC's Finance in brief.")

# Creating a template with proper prompts
template = """Answer the question based only on the following context:
{context}
Question: {Question}
Answer: """

prompt = ChatPromptTemplate.from_template(template)

# from langchain.schema.runnable import RunnablePassthrough # Old import
from langchain_core.runnables import RunnablePassthrough

# RunnablePassthrough forwards the input unchanged in pipelines that require no preprocessing.
# If used in a dictionary-based pipeline, it passes the input as is, or adds extra fields when needed.
rag_chain = (
    {"context": retriever, "Question": RunnablePassthrough()}
    | prompt
)

rag_chain.invoke("Tell about JPMC.")

def docs2str(docs):
  """
  Utility function to convert retrivered docs to proper string format.
  """
  return "\n\n".join(doc.page_content for doc in docs)

# RAG Chain with proper formatted docs being put into the prompt.
# This chain has does process only till the prompt templating - return the prompt with retrieved docs formatted and inserted into them properly.
rag_chain = (
    {"context": retriever | docs2str, "Question": RunnablePassthrough()}
    | prompt
)

rag_chain.invoke("Tell me about JPMC.")

# This rag chain invokes the llm and parses output and displays only the page_content from the json returned
rag_chain = (
    {"context":retriever | docs2str, "Question": RunnablePassthrough()}
    | prompt
    | google_llm
    | output_parser
)

from pprint import pprint

# rag_chain.invoke("Tell me about JPMC")
# question = "When was JPMC founded?"
question = "Tell me about JPMC."
# question = "What does JPMC do?"
response = rag_chain.invoke(question)
pprint(response)

"""# **Conversational RAG**
Handles follow-up questions
"""

from langchain_core.messages import HumanMessage, AIMessage

# To enable conversational RAG, chat_history has to be maintained.
# This list stores Human message and AI message in a list for future use.
chat_history = []
chat_history.extend([
    HumanMessage(content=question),
    AIMessage(content=response)
])

chat_history

from langchain_core.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
    MessagesPlaceholder
)

# Prompt to give answers based on the context and chat history given

# contextualise_q_system_prompt = (
#     "Given a chat history and the latest user question"
#     "which might reference context in the chat history, "
#     "formulate a standalone question which can be understood"
#     "without the chat history. Do NOT answer the question, "
#     "just reformulate it if  needed and otherwise return it as it is"
# )

contextualise_q_system_prompt = (
    "Rephrase the given question and Answer the given question based on the given context and history. Display the rephrased question along with the answer"
)

# Creating a template to insert into the retrieval chain.
contextualise_q_prompt = ChatPromptTemplate.from_messages([
    ("system", contextualise_q_system_prompt),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
])

# Chain built on the contextualised prompt given and used in this chain
contextualise_chain = contextualise_q_prompt | google_llm | output_parser
contextualise_chain.invoke({"input": "Where is this company available?",
                            "chat_history": chat_history})

from langchain.chains import create_history_aware_retriever

# History-aware retriever: Enhances document retrieval by considering past chat history.
# It retrieves relevant documents based on the input query while incorporating conversation context.
history_aware_retriever = create_history_aware_retriever(
    google_llm, retriever, contextualise_q_prompt
)

history_aware_retriever.invoke({"input":"Where is this company available?",
                                "chat_history": chat_history})

from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

# RAG pipeline setup:
# - `qa_prompt`: Defines how the assistant answers using retrieved context and chat history.
# - `question_answer_chain`: Combines retrieved documents to generate responses.
# - `rag_chain`: Links retrieval with the response generator, ensuring context-aware answers.
qa_prompt = ChatPromptTemplate.from_messages([
    ("system", "You are a helpful AI assistant. Use the following context to answer the user's question."),
    ("system", "Context: {context}"),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}")
])

# `create_stuff_documents_chain`: Merges retrieved documents into a structured prompt.
# Ensures all relevant content is formatted before passing it to the LLM for response generation.
# This step improves contextual understanding by consolidating multiple sources into a single coherent input.
question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

# `create_retrieval_chain`: Retrieves relevant documents based on the query
# and passes them to the response generation chain for context-aware answers.
rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)



"""A normal retriever (like a vector store retriever) does not inherently support conversational retrieval because
 it only fetches documents based on the current query—it does not consider past interactions.
 This function - create_retrieval_chain supports conversational retrieval on top of the standard retrieval.
 It takes a retriever and chain made out of a prompt template.
"""

# Invoking the created retrieval chain
result = rag_chain.invoke({"input":"Where are the company's services available?",
                  "chat_history": chat_history})

pprint(result)

"""# **Building a Multi User Chatbot**
Using Sqlite3 database
"""

import sqlite3
from datetime import datetime

DB_NAME = "rag_app.db"

def get_db_connection():
  conn = sqlite3.connect(DB_NAME)
  conn.row_factory = sqlite3.Row
  return conn

def create_application_logs():
  conn = get_db_connection()
  cursor = conn.execute('''CREATE TABLE IF NOT EXISTS application_logs
                        (id INTEGER PRIMARY KEY AUTOINCREMENT,
                        session_id TEXT,
                        user_query TEXT,
                        llm_response TEXT,
                        model TEXT,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP)
                        ''')
  conn.close()

def insert_application_logs(session_id, user_query, llm_response, model):
  conn = get_db_connection()
  conn.execute('INSERT INTO application_logs (session_id, user_query, llm_response, model) VALUES (?, ?, ?, ?)',
               (session_id, user_query, llm_response, model))
  conn.commit()
  conn.close()

def get_chat_history(session_id):
  conn = get_db_connection()
  cursor = conn.execute('SELECT * FROM application_logs WHERE session_id = ? ORDER BY created_at', (session_id,))
  messages = []
  for row in cursor.fetchall():
    messages.extend([
        {"role":"human", "content": row['user_query']},
        {"role":"ai", "content": row['llm_response']}
    ])
  conn.close()
  return messages

# Initialize the database
create_application_logs()

# USER 1

import uuid
session_id = str(uuid.uuid4())

chat_history = get_chat_history(session_id)
print(chat_history)

question1 = "Where is this company available?"
answer1 = rag_chain.invoke({"input": question1, "chat_history": chat_history})['answer']
insert_application_logs(session_id, question1, answer1, google_llm.model)

chat_history.extend([
    {"role":"human", "content": question1},
    {"role":"ai", "content": answer1}
])
pprint(chat_history[-1])

pprint(chat_history)

question2 = "What does it do?"
# Using same session id considers chat history while answering a new question
chat_history = get_chat_history(session_id)
answer2 = rag_chain.invoke({"input": question2, "chat_history": chat_history})['answer']
insert_application_logs(session_id, question2, answer2, google_llm.model)

pprint(answer2)

chat_history = get_chat_history(session_id)
pprint(chat_history)

"""**New User**"""

# Generating a new session key for new user
session_id = str(uuid.uuid4())

chat_history = get_chat_history(session_id)
print(chat_history)

question = "When was JPMC founded?"
answer = rag_chain.invoke({"input": question, "chat_history": chat_history})['answer']
insert_application_logs(session_id, question, answer, google_llm.model)

chat_history.extend([
    {"role":"human", "content": question},
    {"role": "ai", "content": answer}
])

pprint(chat_history)

